---
title: "Linear Regression"
author: "Andrew Min"
date: '2019-05-04'
slug: linear regression
categories:
  - Econometrics
tags:
  - Appiled R
description: ''
thumbnail: ''
---

# 1.Linear regression hypothesis

We often use linear regression to fit the model, but the assumptions behind it are often not too much attention. In fact, if our assumptions are not satisfied for different problems, such as collinearity, autocorrelation, and heteroscedasticity which require different solutions. Therefore, before learning linear regression, it is necessary to understand the six basic hypothesis of linear regression.

- Zero mean assumption:$E(u_i) = 0, t =1, 2,..,n$，that is, the mean value of each disturbance item is 0.

- No autocorrelation assumption:$E(u_iu_j) = 0, i\neq j$，that is, each period of disturbance is irrelevant。Due to
$$cov(u_i, u_j) = E((u_i-E(u_i)(u_j-E(u_j))=E(u_iu_j)(according to assumption 1)$$
So this assumption is equivalent to

$$Cov(u_i, u_j) = 0, i\neq j$$

- Same variance assumption: $E(u_t^2) = \sigma^2, t=1,2,...,n$,that is, the variance of each disturbance is a constant, and the same can be obtained according to assumption 1.：
$$var(u_t) = E(u_t-E(u_t))^2=E(u_t^2)$$

- X is non-random

- $(K+1)<n$，that is, the number of estimated parameters is smaller than the number of samples.

- No collinearity, that is, there is no strict linear relationship between explanatory variables

# 2.Least squares estimation

## 2.1 Least squares principle

To understand the principle of least squares, we use a basic regression model to understand more intuitively.

```{r}
# construct variable x and y
library(ggplot2)
set.seed(1)
x <- rnorm(30)
y <- vector(length = length(x))
for (i in seq_along(x)) {
  y[i] = x[i]*runif(1) + runif(1) + rnorm(1)
}
df <- data.frame(x = x, y = y)

# plot
# set a random line with slope 1 and intercept 0.5
set_slope <- 1
set_intercept <- 0.5
p <- ggplot(aes(x,y),data = df)+geom_point() +geom_abline(slope = set_slope, intercept = set_intercept, color = "red") + geom_pointrange(aes(ymin = y, ymax = (x+0.5)), color = "lightpink") + theme_bw() + geom_text(x = -1.8, y = -0.5, label = "The original line", color = "red") + annotate(geom = "segment", x = -1.8, y = -0.7, xend = -1.8, yend = -1.3, arrow = arrow(unit(10,"inch")), color="red")
p

```

We can see that the data points are relatively scattered, and there is some distance between the fitted line and the point (that is, the downward straight line distance in the figure). How can we make the straight line we fit the best? We will naturally think of the slightest distance in the picture! Yes, this is the right idea, but should we use absolute distance, squared distance, fourth power or what distance? (The distance must be at least a positive number). We finally choose the square distance, to make the formula is the smallest：$$\sum{e_t^2} = \sum{(Y_t - \hat{Y_t})}$$

Here, since the parameters estimated in this way satisfy the unbiasedness and the minimum variance, it will be mentioned later.

## 2.2 Least squares estimation

First we know,

$$\frac{\partial x^T a}{\partial x} = \frac{\partial a^T x}{\partial x} = a $$
$$\frac{\partial x^TAx}{\partial x} = Ax + A^Tx $$

So,

$$||X\beta-Y||_2 = \beta^TX^TX\beta - Y^TX\beta- \beta^TX^TY+Y^TY$$
Derived for $x$,

$$\frac{\partial ||X\beta-Y||_2}{\partial \beta} = 2X^TX\beta - 2X^TY = 0$$

So we get $$\beta = (X^TX)^{-1}X^TY$$

## 2.3 $\beta$ 的性质
Our model is $Y = X\beta + u$ , estimated model is $\hat(Y) = X \hat(\beta)$

### 2.3.1 The mean of $\hat{\beta}$


\begin{align}
\hat{\beta} &= (X^TX)^{-1}X^TY \\
&= (X^TX)^{-1}X^T(X\beta + u)\\
&=  (X^TX)^{-1}X^TX\beta +  (X^TX)^{-1}X^Tu \\
&= \beta + (X^TX)^{-1}X^Tu   \\
&= \beta
\end{align}

So,
$$
E(\hat{\beta}) = E(\beta)
$$

### 2.3.2 The variance of $\hat{\beta}$
Available from the formula above， $\hat{\beta} - \beta = (X^TX)^{-1}X^Tu$
So，

\begin{align}
var(\hat{\beta}) &= E\{((X^TX)^{-1}X^Tu)((X^TX)^{-1}X^Tu)^T\} \\
&= E((X^TX)^{-1}X^Tuu^TX(X^TX)^{-1})\\
&= (X^TX)^{-1}X^TE(uu^T)X(X^TX)^{-1}\\
&= (X^TX)^{-1}X^T\sigma ^2 I_nX(X^TX)^{-1} \\
&= (X^TX)^{-1}\sigma ^2

\end{align}


Where $var(\hat{\beta})$ represents the variance-covariance matrix of $\hat(\beta)$

## 2.4 Gauss-Markov's theorem (minimum variance)
Acorrding to $\hat{\beta}= (X^TX)^{-1}X^TY$ to know that $\hat{\beta}$ can be expressed as the product of a matrix and the $Y$ of the dependent variable observation vector. which is
$$\hat{\beta} = kY$$
Where $ k = (X^TX)^{-1}X^T$ is a $(K+1)*n$ non-random element matrix.
Let $\widetilde{\beta}$ be a linear estimate of $\beta$, ie $\widetilde{\beta} = cY$, in order to take a $\beta$ and get it as before. The estimated amount is compared to the variance.
therefore，


\begin{align}
E(\widetilde{\beta}) &= E(cX\beta + cu) \\
&= cX\beta
\end{align}

If $\widetilde{\beta}$ is an unbiased estimator, it is clear that $cX$ is the unit matrix $I$. The variance of $\widetilde{\beta}$ is:


\begin{align}
var(\widetilde{\beta}) &= var(cX\beta+cu)
&= var(cu) \\
&= c\cdot var(u)\cdot c^T\\
&= \sigma ^2 cc^T
\end{align}

By the nature of k above, you can write $c$ as,

$$
c = (X^TX)^{-1}X^T + D
$$
Available from $cX = I$:

$$
(X^TX)^{-1}X^TX + DX = I
$$

that is $I + DX = I$ ==> $DX=0$

therefore，


\begin{align}
cc^T &= ((X^TX)^{-1}X^T + D)((X^TX)^{-1}X^T + D)^T\\
&= ((X^TX)^{-1}X^T + D)(X(X^TX)^{-1} + D^T)\\
&=(X^TX)^{-1}X^TX(X^TX)^{-1}+ (X^TX)^{-1}X^TD^T + DX(X^TX)^{-1} + DD^T\\
&= (X^TX)^{-1} + DD^T
\end{align}


So，


\begin{align}
var(\widetilde{\beta}) &= \sigma ^2 cc^T\\
&= \sigma ^2((X^TX)^{-1} + DD^T)\\
&= \sigma ^2(X^TX)^{-1} + \sigma ^2DD^T\\
&=var(\hat{\beta}) + \sigma ^2DD^T\\
&\ge var(\hat{\beta})
\end{align}

The last inequality is established because $DD^T$ is a semi-positive matrix. This proves that the OLS estimator $\hat{\beta}$ is the smallest of all linear unbiased estimators.

## 2.5 The distribution of $\beta$

As can be seen from the previous deduction,

$$
\hat{\beta} = \beta + (X^TX)^{-1}X^Tu
$$
This indicates that $\beta$ is a linear combination of $N$ normal distribution variables $u$, and therefore also a normal distribution, as can be seen from the mean and variance derived from the previous,

$$
\hat{\beta} \sim N(\beta, \sigma^2)(X^TX)^{-1}
$$

# 3.Fitted curve

```{r}
model <- lm(y~x)
p + geom_abline(intercept = as.numeric(model$coefficients[1]), slope = as.numeric(model$coefficients[2])) + geom_text(x = 1.2, y = 0.3, label = "The fitting line") + annotate(geom = "segment", x = 1.2, y = 0.5, xend = 1.2, yend = 1, arrow = arrow(unit(10,"inch"), ends = "first"))

# original set line  mse
original_mse <- sum((set_slope * x + set_intercept - y)^2)

# after fitting
fitting_mse <- sum((as.numeric(model$coefficients[2]) * x + as.numeric(model$coefficients[1]) - y)^2)

```

The randomly set slope and intercept get the mse `r original_mse`， The mse obtained by linear regression fitting is `r fitting_mse`
